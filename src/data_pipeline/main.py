import logging from datetime import datetime, timedelta import pandas as pd from .collectors.historical_data import HistoricalDataCollector from .collectors.real_time_data import RealTimeDataCollector from .collectors.odds_data import OddsDataCollector from .collectors.contextual_data import ContextualDataCollector from .processors.data_processor import DataProcessor from .storage.database import DatabaseManager  logging.basicConfig(level=logging.INFO) logger = logging.getLogger(__name__)  class DataPipeline:     def __init__(self):         self.historical_collector = HistoricalDataCollector()         self.real_time_collector = RealTimeDataCollector()         self.odds_collector = OddsDataCollector()         self.contextual_collector = ContextualDataCollector()         self.processor = DataProcessor()         self.db = DatabaseManager()          def run_historical_collection(self, seasons):         """Collect historical data for specified seasons"""         logger.info(f"Starting historical data collection for seasons: {seasons}")                  for season in seasons:             try:                 data = self.historical_collector.collect_season_data(season)                 processed_data = self.processor.process_historical_data(data)                 self.db.save_historical_data(processed_data, season)                 logger.info(f"Successfully collected data for season {season}")             except Exception as e:                 logger.error(f"Error collecting data for season {season}: {e}")          def run_daily_collection(self):         """Collect daily data including real-time and contextual data"""         logger.info("Starting daily data collection")                  # Collect real-time data         try:             real_time_data = self.real_time_collector.collect_current_games()             processed_real_time = self.processor.process_real_time_data(real_time_data)             self.db.save_real_time_data(processed_real_time)         except Exception as e:             logger.error(f"Error collecting real-time data: {e}")                  # Collect odds data         try:             odds_data = self.odds_collector.collect_odds()             processed_odds = self.processor.process_odds_data(odds_data)             self.db.save_odds_data(processed_odds)         except Exception as e:             logger.error(f"Error collecting odds data: {e}")                  # Collect contextual data         try:             contextual_data = self.contextual_collector.collect_contextual_data()             processed_contextual = self.processor.process_contextual_data(contextual_data)             self.db.save_contextual_data(processed_contextual)         except Exception as e:             logger.error(f"Error collecting contextual data: {e}")          def run_processing_pipeline(self):         """Run full data processing pipeline"""         logger.info("Starting data processing pipeline")                  try:             # Extract raw data             historical_data = self.db.extract_historical_data()             real_time_data = self.db.extract_real_time_data()             contextual_data = self.db.extract_contextual_data()                          # Transform and merge data             merged_data = self.processor.merge_datasets(                 historical_data, real_time_data, contextual_data             )                          # Create features             featured_data = self.processor.create_features(merged_data)                          # Load processed data             self.db.save_processed_data(featured_data)             logger.info("Data processing pipeline completed successfully")                      except Exception as e:             logger.error(f"Error in data processing pipeline: {e}")  if __name__ == "__main__":     pipeline = DataPipeline()          # Example usage:     # pipeline.run_historical_collection([2022, 2023])     # pipeline.run_daily_collection()     # pipeline.run_processing_pipeline()
